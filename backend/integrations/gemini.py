# backend/integrations/gemini.py
"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Google Gemini AI
"""
from google import genai
from core.config import GEMINI_API_KEY, GEMINI_MODEL

async def ask_gemini(prompt: str, context: str = "", **kwargs) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ Gemini AI
    
    Args:
        prompt: –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (max_tokens, temperature, etc.)
    
    Returns:
        str: –û—Ç–≤–µ—Ç –æ—Ç AI –∏–ª–∏ fallback —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    client = genai.Client(api_key=GEMINI_API_KEY)
    full_prompt = f"{context}\n\n{prompt}" if context else prompt

    # Map max_tokens to max_output_tokens for Gemini
    generation_config = {}
    if 'max_tokens' in kwargs:
        generation_config['max_output_tokens'] = kwargs.pop('max_tokens')

    # Add any other kwargs to generation_config
    generation_config.update(kwargs)

    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=full_prompt,
            config=generation_config if generation_config else None
        )
        return response.text.strip()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑! üòä"